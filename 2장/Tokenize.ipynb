{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/1 목(Tokenize).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUznyNRWu6gw6Umm3OSp74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_1_%EB%AA%A9(Tokenize).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CTRh4MtvIOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3d4ac5-f2ca-4e48-d7f0-ff3cd12b869e"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"2-1.nltk.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/15HXS3XRrKyBGzAFNftvtEfD5FF_8l0lH\n",
        "\"\"\"\n",
        "\n",
        "# 2장. 자연어 처리 개발 준비. 토큰나이징 (p.63)\n",
        "import nltk # 토큰을 단어들로 쪼갤때 필요함\n",
        "nltk.download('punkt')      # '/root/nltk_data/tokenizers'\n",
        "nltk.download('stopwords')  # 불용어 목록\n",
        "\n",
        "sentence = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of computer science, information engineering, \n",
        "and artificial intelligence concerned with the interactions between computers and human (natural) languages, \n",
        "in particular how to program computers to process and analyze large amounts of natural language data.\n",
        "\"\"\"\n",
        "# token(기본단위) 단어를 나눌 수도 있다. 단어가 될 수도 있지만 단어가 아닐 수도 있다. 단어를 쪼갤 수 있다.\n",
        "# 문장을 수치데이터로 변환해야하는데 그때 필요한게 \"tokenize\"라고 한다. \n",
        "word_tok = nltk.word_tokenize(sentence) \n",
        "print(word_tok)\n",
        "\n",
        "\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of computer science, information engineering, \n",
        "and artificial intelligence concerned with the interactions between computers and human (natural) languages, \n",
        "in particular how to program computers to process and analyze large amounts of natural language data. \n",
        "Challenges in natural language processing frequently involve speech recognition, natural language understanding, \n",
        "and natural language generation.\n",
        "\"\"\"\n",
        "# sent = sentence, 문장 단위로 쪼개는것\n",
        "# 문서를 문장 단위로 쪼개는 것\n",
        "sent_tok = nltk.sent_tokenize(text)\n",
        "print(sent_tok)\n",
        "\n",
        "sent_tok[0]  # 첫번째 문장\n",
        "#sent_tok[1] # 2번째 문장\n",
        "\n",
        "# 텍스트에서 단어 분리\n",
        "word_tokens = [nltk.word_tokenize(x) for x in nltk.sent_tokenize(text)]\n",
        "# 2차원 구조로 나열된다. \n",
        "word_tokens[0]\n",
        "\n",
        "# stop words 제거\n",
        "# 불용어\n",
        "stopwords = nltk.corpus.stopwords.words('english')  # 등록된 stop word\n",
        "# coupus 수많은 언어 data -> 기계학습 \n",
        "stopwords  # list로 들어있다. \n",
        "stopwords.append(',') # : , 쉼표 제거 \n",
        "\n",
        "# text를 문장으로 쪼개고, 단어로 쪼갠다. \n",
        "# 불용어를 제거 하는 방법\n",
        "all_tokens = []\n",
        "for sent in word_tokens:\n",
        "    all_tokens.append([word for word in sent if word.lower() not in stopwords])\n",
        "all_tokens\n",
        "\n",
        "# Stemming 원형으로 변환\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "for word in ['working', 'works', 'worked']: # 3개의 단어를 하나의 단어로 받으면 원형으로 받아들인다. \n",
        "    print(stemmer.stem(word))\n",
        "\n",
        "# Lemmatization\n",
        "# 사전을 이용해서 원형으로 변환하는 방법\n",
        "# 품사에 따라서 변환\n",
        "# n: 명사  \n",
        "# v: 동사\n",
        "# a: 형용사\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "for word in ['working', 'works', 'worked']:\n",
        "    print(lemma.lemmatize(word, 'v')) # v: 동사\n",
        "\n",
        "for word in ['happier', 'happiest']:\n",
        "    print(lemma.lemmatize(word, 'a'))  # a: 형용사\n",
        "\n",
        "########################################\n",
        "### 굉장히 많이 사용함 (현재 NLP의 필수사항) ###\n",
        "########################################\n",
        "# 사전 (vocabulary) 생성\n",
        "# word to index\n",
        "# 딕셔너리가 hash(data base 내부 엔진의 구조) 구조로 이루어져있다. 충돌이 발생하는 단점이 있다(collisim)\n",
        "# hash function 수치화\n",
        "# hash는 DB영역과 암호화 영역에 많이 쓰인다. \n",
        "# 어떤 단어와 숫자를 수치화 시킬 수 있는 방법 : hash. Ex) hash('naturl') % 60\n",
        "word2idx = {}  # 단어를 숫자로 만들겠다. 딕셔너리로 만들어야한다. Ex) key:\"love\", value: 몇번째 위치한뜻\n",
        "n_idx = 0\n",
        "for sent in all_tokens:\n",
        "    for word in sent:\n",
        "        if word.lower() not in word2idx:  # 사전에 등록되어 있지 않으면 \n",
        "            word2idx[word.lower()] = n_idx  # 사전에 등록해줘라 / word2idx에 key값을 입력해주고 n_idx에 value값을 넣어줘라\n",
        "            n_idx += 1\n",
        "# 2번째 사전            \n",
        "idx2word = {v:k for k, v in word2idx.items()} # 반대경우  \n",
        "# k: key v: value\n",
        "# Ex) idx2word[0] = \"natural\"값을 보여준다. 숫자 --> key(단어뜻)으로 보여준다. \n",
        "\n",
        "word2idx  # NLP에 사용하는 \"사전\" 굉장히 중요하고 기본이고 많이 사용한다. \n",
        "word2idx.items() # idx2word의 값을 보여준다. \n",
        "\n",
        "\n",
        "# text를 사전의 index로 표현\n",
        "# 사전을 숫자로 표현하였다. \n",
        "# 단어를 index 숫자로 표현하였다. \n",
        "# 쓰이려면 사전과 숫자로 표현된 데이터가 쌍으로 다녀야 한다. 한쪽만 있으면 안된다. \n",
        "# 하나의 세트로 이해하면 쉽다. \n",
        "text_idx = []\n",
        "for sent in all_tokens:\n",
        "    sent_idx = []\n",
        "    for word in sent:\n",
        "        sent_idx.append(word2idx[word.lower()])\n",
        "    text_idx.append(sent_idx)\n",
        "print(text_idx[0])\n",
        "\n",
        "# text_idx를 다시 단어로 표시\n",
        "text = []\n",
        "for sent_idx in text_idx:\n",
        "    sent = []\n",
        "    for word_idx in sent_idx:\n",
        "        sent.append(idx2word[word_idx])\n",
        "    text.append(sent)\n",
        "print(text[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
