{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/20 화(NPLM-LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNY4AsQGqldrTOgeNTLLCOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_20_%ED%99%94(NPLM_LSTM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cME6PsX7C38l"
      },
      "source": [
        "# A Neural Probabilistic Language Model (NPLM)\n",
        "#\n",
        "# NPLM 논문 : Yoshua Bengio, et. al., 2003, A Neural Probabilistic Language Model\n",
        "# 코드 구현 : blog.naver.com/chunjein, 2021.03.22\n",
        "# -------------------------------------------------------------------------------\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = [\"The cat is walking in the bedroom\",\n",
        "        \"A dog was running in a room\",\n",
        "        \"The cat is running in a room\",\n",
        "        \"A dog is walking in a bedroom\",\n",
        "        \"The dog was walking in the room\"]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "word2idx = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# sequences 뒤에 <EOS>를 추가한다.\n",
        "word2idx_len = len(word2idx)\n",
        "word2idx['<EOS>'] = word2idx_len + 1  # end of sentence 추가\n",
        "idx2word = {v: k for (k, v) in word2idx.items()}\n",
        "sequences = [s + [word2idx['<EOS>']] for s in sequences]\n",
        "\n",
        "sequences\n",
        "\n",
        "def prepare_sentence(seq, maxlen):\n",
        "    # Pads seq and slides windows\n",
        "    x = []\n",
        "    y = []\n",
        "    for i, w in enumerate(seq[1:], 1):\n",
        "        x.append(pad_sequences([seq[:i]], maxlen=maxlen - 1)[0])\n",
        "        y.append(w)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# 학습 데이터를 생성한다.\n",
        "maxlen = max([len(s) for s in sequences])\n",
        "x = []\n",
        "y = []\n",
        "for seq in sequences:\n",
        "    x_, y_ = prepare_sentence(seq, maxlen)\n",
        "    x += x_\n",
        "    y += y_\n",
        "    \n",
        "x_train = np.array(x)\n",
        "y_train = np.array(y)\n",
        "\n",
        "\n",
        "# NPLM 모델을 생성한다.\n",
        "EMB_SIZE = 8\n",
        "VOCAB_SIZE = len(word2idx) + 1\n",
        "x_input = Input(batch_shape = (None, x_train.shape[1]))\n",
        "x_embed = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE, name='emb')(x_input)\n",
        "\n",
        "x_lstm = LSTM(10)(x_embed)\n",
        "y_output = Dense(VOCAB_SIZE, activation = 'softmax')(x_lstm)\n",
        "\n",
        "model = Model(x_input, y_output)     # 학습, 예측용 모델\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.01))\n",
        "model.summary()\n",
        "\n",
        "# 모델을 학습한다.\n",
        "hist = model.fit(x_train, y_train, epochs=300, verbose=0)\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "C = model.get_layer('emb').get_weights()[0]\n",
        "\n",
        "# 한 단어의 워드 벡터를 조회한다.\n",
        "word = 'dog'\n",
        "w_idx = word2idx[word]\n",
        "wv = C[w_idx, :]  # look-up\n",
        "print('\\n단어 :', word)\n",
        "print(np.round(wv, 3))\n",
        "\n",
        "\n",
        "def get_prediction(model, sent):\n",
        "    x = tokenizer.texts_to_sequences(sent)[0]\n",
        "    x = pad_sequences([x], maxlen=maxlen - 1)[0]\n",
        "    x = np.array(x).reshape(1, -1)\n",
        "    return model.predict(x)[0]\n",
        "\n",
        "# 주어진 문장 다음에 나올 단어를 예측한다.\n",
        "x_test = ['A dog is walking in a']\n",
        "p = get_prediction(model, x_test)\n",
        "n = np.argmax(p)\n",
        "prob = p[n]\n",
        "next_word = idx2word[n]\n",
        "print(\"\\n{} --> '{}', probability = {:.2f}%\".format(x_test, next_word, prob * 100))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
