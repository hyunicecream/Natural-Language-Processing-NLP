{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/21 수(skipgram-sub,negative sampling).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1tTl-DY5teqFD5qDEVYsaOwKH2D7I9VnP",
      "authorship_tag": "ABX9TyMLINmYudsjWFAOb4v+KFNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_21_%EC%88%98(skipgram_sub%2Cnegative_sampling).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dSFUMdk1vGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ec89a8-38c4-4063-ee95-dd0c0af646de"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"6-7.SGNS.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1cA_8CpQWqiCuYpMSbWPpb1oYJjAHygLl\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dot, Activation, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# 전처리가 완료된 한글 코퍼스를 읽어온다.\n",
        "# %cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "with open('/content/drive/MyDrive/머신러닝/자연어/konovel_preprocessed.pkl', 'rb') as f:\n",
        "    sent_list = pickle.load(f)\n",
        "\n",
        "max_word = 10000\n",
        "tokenizer = Tokenizer(num_words = max_word, oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(sent_list)\n",
        "sent_idx = tokenizer.texts_to_sequences(sent_list)\n",
        "word2idx = {k:v for (k, v) in list(tokenizer.word_index.items())[:max_word]}\n",
        "idx2word = {v:k for (k, v) in word2idx.items()}\n",
        "\n",
        "# 학습 데이터 생성\n",
        "n_gram = 5      # 5-grams\n",
        "target = []    # target word\n",
        "context = []   # context word\n",
        "\n",
        "# positive data\n",
        "for sent in sent_idx:\n",
        "    if len(sent) < n_gram:\n",
        "        continue\n",
        "\n",
        "    # 5-gram\n",
        "    for w1, w2, w3, w4, w5 in nltk.ngrams(sent, n_gram):\n",
        "        target.extend([w3, w3, w3, w3])   # target\n",
        "        context.extend([w1, w2, w4, w5])   # context\n",
        "\n",
        "# Subsampling of frequent words.\n",
        "# [1]의 후속 논문인 [2]에 소개된 subsampling 기법을 적용한다.\n",
        "def sub_sampling(x, y):\n",
        "    # x, y를 합친다.\n",
        "    data = np.hstack([x, y])\n",
        "    \n",
        "    # data = (x, y) 쌍을 shuffling 한다.\n",
        "    np.random.shuffle(data)\n",
        "    \n",
        "    # data의 x 값을 기준으로 subsampling을 적용한다.\n",
        "    d = np.empty(shape = (0, 2), dtype=np.int32)\n",
        "    for x_set in set(data[:, 0]):\n",
        "        x_tmp = data[np.where(data[:, 0] == x_set)]\n",
        "\n",
        "        fw = 1e-8 + x_tmp.shape[0] / data.shape[0]\n",
        "        pw = np.sqrt(1e-5 / fw)              # 남겨야할 비율\n",
        "        cw = np.int(x_tmp.shape[0] * pw) + 1 # 남겨야할 개수 - subsampling 개수\n",
        "        d = np.vstack([d, x_tmp[:cw]])\n",
        "\n",
        "    # d[:, 1]은 0,1,2,... 순으로 되어 있어서 다시 한번 shuffle 한다.\n",
        "    np.random.shuffle(d)\n",
        "    return list(d[:, 0].reshape(-1)), list(d[:, 1].reshape(-1))\n",
        "\n",
        "def train_data(t, c, voc_size):\n",
        "    # subsampling\n",
        "    x_target_pos, x_context_pos = sub_sampling(np.array(t).reshape(-1,1), np.array(c).reshape(-1,1))\n",
        "    y_train_pos = [1] * len(x_target_pos)\n",
        "\n",
        "    # negative sampling. random이 오래 걸려서 아래처럼 일괄처리함.\n",
        "    ns_k = 2\n",
        "    x_target_neg = []     # negative target word\n",
        "    x_context_neg = []   # negative context word\n",
        "    for k in range(ns_k):\n",
        "        r = np.random.choice(range(1, voc_size), len(x_target_pos))\n",
        "        x_target_neg.extend(x_target_pos.copy())\n",
        "        x_context_neg.extend(list(r).copy())\n",
        "    y_train_neg = [0] * len(x_target_neg)\n",
        "\n",
        "    # positive + negative\n",
        "    x_target = x_target_pos + x_target_neg\n",
        "    x_context = x_context_pos + x_context_neg\n",
        "    y_train = y_train_pos + y_train_neg   \n",
        "\n",
        "    # shuffling\n",
        "    x_target, x_context, y_train = shuffle(x_target, x_context, y_train)\n",
        "\n",
        "    # list --> array 변환\n",
        "    x_target = np.array(x_target).reshape(-1, 1)\n",
        "    x_context = np.array(x_context).reshape(-1, 1)\n",
        "    y_train = np.array(y_train).reshape(-1, 1)\n",
        "\n",
        "    return x_target, x_context, y_train\n",
        "\n",
        "VOC_SIZE = len(word2idx) + 1\n",
        "EMB_SIZE = 32\n",
        "\n",
        "LOAD_MODEL = False\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    # 학습된 모델을 읽어온다.\n",
        "    model = load_model(\"/content/drive/MyDrive/머신러닝/자연어/skipgram_model.h5\")    \n",
        "else:\n",
        "    x_input_t = Input(batch_shape=(None, 1))\n",
        "    x_input_c = Input(batch_shape=(None, 1))\n",
        "\n",
        "    SharedEmb = Embedding(VOC_SIZE, EMB_SIZE, name='emb_vec')\n",
        "    x_emb_t = SharedEmb(x_input_t)\n",
        "    x_emb_c = SharedEmb(x_input_c)\n",
        "\n",
        "    y_output = Dot(axes=(2,2))([x_emb_t, x_emb_c])\n",
        "    y_output = Activation('sigmoid')(y_output)\n",
        "\n",
        "    model = Model([x_input_t, x_input_c], y_output)\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "for i in range(1):\n",
        "    x_target, x_context, y_train = train_data(target, context, len(word2idx))\n",
        "    model.fit([x_target, x_context], y_train, batch_size=10240, epochs=10)\n",
        "\n",
        "# 학습 결과를 저장해 둔다.\n",
        "model.save(\"/content/drive/MyDrive/머신러닝/자연어/skipgram_model.h5\")\n",
        "\n",
        "wv = model.get_layer('emb_vec').get_weights()[0]\n",
        "\n",
        "# 주어진 단어의 word2vec 확인\n",
        "def get_word2vec(word, wv):\n",
        "    if word in word2idx:\n",
        "        x = np.array(word2idx[word]).reshape(-1,1)\n",
        "    else:\n",
        "        x = np.array(word2idx['<OOV>']).reshape(-1,1)\n",
        "    return wv[x, :][0][0]\n",
        "\n",
        "word2vec = get_word2vec('아버지', wv)\n",
        "print(np.round(word2vec, 4))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "emb_vec (Embedding)             (None, 1, 32)        320032      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 1)         0           emb_vec[0][0]                    \n",
            "                                                                 emb_vec[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 1, 1)         0           dot[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 320,032\n",
            "Trainable params: 320,032\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "79/79 [==============================] - 2s 16ms/step - loss: 0.6931\n",
            "Epoch 2/10\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.6916\n",
            "Epoch 3/10\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.6854\n",
            "Epoch 4/10\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.6733\n",
            "Epoch 5/10\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.6633\n",
            "Epoch 6/10\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.6577\n",
            "Epoch 7/10\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.6539\n",
            "Epoch 8/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.6505\n",
            "Epoch 9/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.6466\n",
            "Epoch 10/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.6423\n",
            "[-0.2748  0.01   -0.3416  0.0009  0.0333  0.3179 -0.2742 -0.5236 -0.369\n",
            "  0.3907 -0.3612 -0.0828  0.1509 -0.3518 -0.002   0.4445 -0.4436  0.2799\n",
            "  0.2026  0.4654 -0.3927 -0.2823 -0.1191  0.441  -0.0965  0.3959  0.3553\n",
            " -0.342  -0.2811  0.1767  0.1904  0.2853]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR18g_dy8mDW",
        "outputId": "5be14528-047e-4340-b8ab-76ddfde5a04d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 21 02:48:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW4uCzAu8qdk",
        "outputId": "89fe05fd-f766-4fe8-e6a2-6f29dc04e052"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
