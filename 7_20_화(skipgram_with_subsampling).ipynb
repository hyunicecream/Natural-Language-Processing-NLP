{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/20 화(skipgram with subsampling).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPz8zqjL5V3nGeHNfL8swF4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_20_%ED%99%94(skipgram_with_subsampling).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb4huq0nBvCr"
      },
      "source": [
        "# Skipgram-2 : Skipgram with subsampling.\n",
        "#\n",
        "# Skipgram으로 한글 코퍼스를 학습하고,\n",
        "# 1) 워드 벡터를 구해보고,\n",
        "# 2) 단어간 의미적 유사도를 확인한다.\n",
        "#\n",
        "# 관련 논문 : [1] Tomas Mikolov, et. al., 2013, Efficient Estimation of Word \n",
        "#                 Representations in Vector Space\n",
        "#            [2] Tomas Mikolov, et. al., 2013, distributed representations of words \n",
        "#                and phrases and their compositionality          \n",
        "# 코드 구현 : blog.naver.com/chunjein, 2021.03.23\n",
        "# ----------------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "# 전처리가 완료된 한글 코퍼스를 읽어온다.\n",
        "with open('../data/konovel_preprocessed.pickle', 'rb') as f:\n",
        "    sent_list = pickle.load(f)\n",
        "\n",
        "max_word = 10000\n",
        "tokenizer = Tokenizer(num_words = max_word, oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(sent_list)\n",
        "sent_idx = tokenizer.texts_to_sequences(sent_list)\n",
        "word2idx = {k:v for (k, v) in list(tokenizer.word_index.items())[:max_word]}\n",
        "idx2word = {v:k for (k, v) in word2idx.items()}\n",
        "\n",
        "# 5-gram으로 학습 데이터를 생성한다.\n",
        "x_data = []     # 입력 데이터\n",
        "y_data = []     # 출력 데이터\n",
        "for sentence in sent_idx:\n",
        "    # 5-gram으로 주변 단어들을 묶는다. 가운데 단어와 다른 단어들의 쌍을 만든다.\n",
        "    contexts = nltk.ngrams(sentence, 5)\n",
        "    pairs = [[(c[2], c[0]), (c[2], c[1]), (c[2], c[3]), (c[2], c[4])] for c in contexts]\n",
        "    for pair in pairs:\n",
        "        for p in pair:\n",
        "            if word2idx['<OOV>'] not in p:  # oov가 포함된 쌍은 제외한다.\n",
        "                x_data.append(p[0])\n",
        "                y_data.append(p[1])\n",
        "\n",
        "x = np.array(x_data).reshape(-1, 1)\n",
        "y = np.array(y_data).reshape(-1, 1)\n",
        "x.shape, y.shape\n",
        "\n",
        "# Subsampling of frequent words.\n",
        "# [1]의 후속 논문인 [2]에 소개된 subsampling 기법을 적용한다.\n",
        "# x : target, y : context\n",
        "def sub_sampling(x, y):\n",
        "    # x, y를 합친다.\n",
        "    data = np.hstack([x, y])\n",
        "    \n",
        "    # data = (x, y) 쌍을 shuffling 한다.\n",
        "    np.random.shuffle(data)\n",
        "    \n",
        "    # data의 x 값을 기준으로 subsampling을 적용한다.\n",
        "    d = np.empty(shape = (0, 2), dtype=np.int32)\n",
        "    for x_set in set(data[:, 0]):\n",
        "        x_tmp = data[np.where(data[:, 0] == x_set)]\n",
        "\n",
        "        fw = 1e-8 + x_tmp.shape[0] / data.shape[0]\n",
        "        pw = np.sqrt(1e-5 / fw)              # 남겨야할 비율\n",
        "        cw = np.int(x_tmp.shape[0] * pw) + 1 # 남겨야할 개수 - subsampling 개수\n",
        "        d = np.vstack([d, x_tmp[:cw]])\n",
        "\n",
        "    # d[:, 1]은 0,1,2,... 순으로 되어 있어서 다시 한번 shuffle 한다.\n",
        "    np.random.shuffle(d)\n",
        "    return d[:, 0].reshape(-1, 1), d[:, 1].reshape(-1, 1)\n",
        "\n",
        "# skipgram 모델을 생성한다.\n",
        "VOCAB_SIZE = len(word2idx) + 1\n",
        "EMB_SIZE = 64\n",
        "LOAD_MODEL = True\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    # 학습된 모델을 읽어온다.\n",
        "    model = load_model(\"../data/skipgram_model.h5\")    \n",
        "else:\n",
        "    x_input = Input(batch_shape = (None, 1))\n",
        "    wv_layer = Embedding(VOCAB_SIZE, EMB_SIZE, name='emb_vec')(x_input)\n",
        "    y_output = Dense(VOCAB_SIZE, activation='softmax')(wv_layer)\n",
        "    \n",
        "    model = Model(x_input, y_output)     # 학습용 모델\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.001))\n",
        "model.summary()\n",
        "\n",
        "# 학습.\n",
        "for i in range(1):\n",
        "    x_train, y_train = sub_sampling(x, y)\n",
        "    model.fit(x_train, y_train, batch_size=1024, epochs=10)\n",
        "    \n",
        "# 학습 결과를 저장해 둔다.\n",
        "model.save(\"../data/skipgram_model.h5\")\n",
        "\n",
        "# 어휘 사전인 word2idx도 저장해 둔다.\n",
        "with open('../data/skipgram_word2idx.pkl', 'wb') as f:\n",
        "    pickle.dump([word2idx, idx2word], f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "# 주어진 단어의 주변 단어 (context) 확인\n",
        "def get_contexts(word, top_n=10):\n",
        "    if word in word2idx:\n",
        "        x = np.array(word2idx[word]).reshape(-1,1)\n",
        "    else:\n",
        "        x = np.array(word2idx['<OOV>']).reshape(-1,1)\n",
        "\n",
        "    context_prob = model.predict(x)[0][0]\n",
        "    top_idx = np.argsort(context_prob)[::-1][:top_n]\n",
        "    return [idx2word[i] for i in top_idx]\n",
        "\n",
        "context = get_contexts('사랑')\n",
        "print(context)\n",
        "\n",
        "wv = model.get_layer('emb_vec').get_weights()[0]\n",
        "\n",
        "# 주어진 단어의 word2vec 확인\n",
        "def get_word2vec(word, wv):\n",
        "    if word in word2idx:\n",
        "        x = np.array(word2idx[word]).reshape(-1,1)\n",
        "    else:\n",
        "        x = np.array(word2idx['<OOV>']).reshape(-1,1)\n",
        "    return wv[x, :][0][0]\n",
        "\n",
        "word2vec = get_word2vec('아버지', wv)\n",
        "print(np.round(word2vec, 4))\n",
        "\n",
        "# 단어간 유사도 측정\n",
        "doctor = get_word2vec('의사', wv)\n",
        "patient = get_word2vec('환자', wv)\n",
        "sea = get_word2vec('김치', wv)\n",
        "\n",
        "print('\\n의사 - 환자 :', np.round(cosine_similarity([doctor, patient])[0, 1], 4))\n",
        "print('의사 - 김치 :', np.round(cosine_similarity([doctor, sea])[0, 1], 4))\n",
        "\n",
        "father = get_word2vec('아빠', wv)\n",
        "mother = get_word2vec('엄마', wv)\n",
        "daughter = get_word2vec('딸', wv)\n",
        "\n",
        "print('\\n아빠 - 딸 :', np.round(cosine_similarity([father, daughter])[0, 1], 4))\n",
        "print('엄마 - 딸 :', np.round(cosine_similarity([mother, daughter])[0, 1], 4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}