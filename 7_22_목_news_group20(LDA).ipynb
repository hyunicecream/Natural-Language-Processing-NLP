{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/22 목-news_group20(LDA).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10fhBhoGXWsIOw5MDX7wWLaTBUOXOaDPR",
      "authorship_tag": "ABX9TyPx79Nk+uesD8qluQknh4pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_22_%EB%AA%A9_news_group20(LDA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G8qQk1tGJOZ",
        "outputId": "b0cfa532-9f2d-4831-95da-f56d32e4071f"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"news_group20(LDA).ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1j8yHHgLQo4VRTZXzryEEDhiBMJaA7_T7\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# Latent Dirichlet Allocation (LDA)\n",
        "# ---------------------------------\n",
        "import numpy as np\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "# %cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# 전처리가 완료된 한글 코퍼스를 읽어온다.\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/pickle/newsgroup20.pkl', 'rb') as f:\n",
        "    subject, text, target = pickle.load(f)\n",
        "\n",
        "n_target = len(set(target))\n",
        "\n",
        "# TF-IDF matrix를 생성한다.\n",
        "tf_vector = TfidfVectorizer(max_features = 500)\n",
        "tfidf = tf_vector.fit_transform(text)\n",
        "print(tfidf.shape)\n",
        "\n",
        "vocab = tf_vector.get_feature_names()\n",
        "print(vocab[:20])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['acceler', 'accept', 'access', 'address', 'advertis', 'advic', 'agnost', 'albican', 'algorithm', 'almost', 'altern', 'amend', 'american', 'ancient', 'anecdot', 'anim', 'announc', 'anoth', 'answer', 'anti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4vzNSEmJaPG",
        "outputId": "488db6d8-75d0-4b3e-e8fb-1ccd044b696c"
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA)\n",
        "# ---------------------------------\n",
        "# Return 값이 Document-Topic distribution이다.\n",
        "# iteration 횟수가 max_iter까지 가면 아직 수렴하지 않은 것이다.\n",
        "# 아직 수렴하지 않은 경우 mat_iter를 증가시켜야 한다.\n",
        "model = LDA(n_components = n_target, \n",
        "            learning_method='online', \n",
        "            evaluate_every=5, \n",
        "            max_iter=1000, \n",
        "            verbose=1)\n",
        "\n",
        "doc_topic = model.fit_transform(tfidf)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 1 of max_iter: 1000\n",
            "iteration: 2 of max_iter: 1000\n",
            "iteration: 3 of max_iter: 1000\n",
            "iteration: 4 of max_iter: 1000\n",
            "iteration: 5 of max_iter: 1000, perplexity: 733.4263\n",
            "iteration: 6 of max_iter: 1000\n",
            "iteration: 7 of max_iter: 1000\n",
            "iteration: 8 of max_iter: 1000\n",
            "iteration: 9 of max_iter: 1000\n",
            "iteration: 10 of max_iter: 1000, perplexity: 732.2986\n",
            "iteration: 11 of max_iter: 1000\n",
            "iteration: 12 of max_iter: 1000\n",
            "iteration: 13 of max_iter: 1000\n",
            "iteration: 14 of max_iter: 1000\n",
            "iteration: 15 of max_iter: 1000, perplexity: 731.9864\n",
            "iteration: 16 of max_iter: 1000\n",
            "iteration: 17 of max_iter: 1000\n",
            "iteration: 18 of max_iter: 1000\n",
            "iteration: 19 of max_iter: 1000\n",
            "iteration: 20 of max_iter: 1000, perplexity: 731.8462\n",
            "iteration: 21 of max_iter: 1000\n",
            "iteration: 22 of max_iter: 1000\n",
            "iteration: 23 of max_iter: 1000\n",
            "iteration: 24 of max_iter: 1000\n",
            "iteration: 25 of max_iter: 1000, perplexity: 731.7680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcP5qhEbJ4aW",
        "outputId": "4a340949-2a6d-4b72-b533-0bb88750b338"
      },
      "source": [
        "# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)\n",
        "for i in range(10):\n",
        "    print('문서-{:d} : topic = {:02d}, target = {:02d}'.format(i, np.argmax(doc_topic[i:(i+1), :][0]), target[i]))\n",
        "\n",
        "text[0], text[7]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문서-0 : topic = 19, target = 17\n",
            "문서-1 : topic = 09, target = 00\n",
            "문서-2 : topic = 16, target = 17\n",
            "문서-3 : topic = 05, target = 11\n",
            "문서-4 : topic = 00, target = 10\n",
            "문서-5 : topic = 13, target = 15\n",
            "문서-6 : topic = 06, target = 04\n",
            "문서-7 : topic = 19, target = 17\n",
            "문서-8 : topic = 13, target = 13\n",
            "문서-9 : topic = 09, target = 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('isra terror', 'land peac negotiatian')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9w8PSnCKEuP",
        "outputId": "2914486f-90ad-43b4-9e58-6e8c07e89dd0"
      },
      "source": [
        "# topic_term 행렬에서 topic 별로 중요 단어를 표시한다\n",
        "topic_term = model.components_\n",
        "for i in range(len(topic_term)):\n",
        "    #idx = np.flipud(topic_term[i].argsort())[:10]\n",
        "    idx= topic_term[i].argsort()[::-1][:10]  #sort 한 다음에 뒤짚어라\n",
        "    print('토픽-{:2d} : '.format(i+1), end='')\n",
        "    for n in idx:\n",
        "        print('{:s} '.format(vocab[n]), end='')\n",
        "    print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "토픽- 1 : time islam simm next rushdi mormon concept cryptographi jew automot \n",
            "토픽- 2 : window driver look video mous appl name order memori set \n",
            "토픽- 3 : help need want info atheist opinion fast kill jesu gateway \n",
            "토픽- 4 : problem armenian sourc burn genocid dividian ranch survivor scienc firearm \n",
            "토픽- 5 : hell predict playoff insur point find april doubl love speed \n",
            "토픽- 6 : chip clipper work announc manag plu post white modem hous \n",
            "토픽- 7 : team hezbollah imag motorcycl traffic child consid star report women \n",
            "토픽- 8 : good code read polit pleas tap mani cheap thought warn \n",
            "토픽- 9 : forsal softwar font trade screen health philli homosexu boom ticket \n",
            "토픽-10 : question card clinton bike use quadra date electron nation press \n",
            "토픽-11 : updat graphic list part comput line number packag beat xterm \n",
            "토픽-12 : christian file color arrog book waco program best batf race \n",
            "토픽-13 : final ride motif note record seri differ run davidian mask \n",
            "토픽-14 : moral year like american sensit superstit christian free first give \n",
            "토픽-15 : drive monitor hockey vandal messag know letter presid open hall \n",
            "토픽-16 : scsi hard secret algorithm crypto avail could escrow turkish protect \n",
            "토픽-17 : game system israel long expans centri studi isra upgrad percentag \n",
            "토픽-18 : chang request right data disk printer answer biblic licens machin \n",
            "토픽-19 : space control news advic realli tape bill real rise group \n",
            "토픽-20 : sale player basebal jewish price power mail terror station isra \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}