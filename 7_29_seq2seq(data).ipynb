{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/29 seq2seq(data).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XSmvW74RmVbutIGaFshlcNMaguHZl1Xn",
      "authorship_tag": "ABX9TyMQkQCNHaxZ0LeLYZSiknES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_29_seq2seq(data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeYFDLrpGJ-G",
        "outputId": "75970211-35f4-490e-f6df-e39504707eca"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsfMoRBxFSFi"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"7-1.seq2seq(data).ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1BqmWF57Igo6SYvSEPm22LAvTa5kxrrIA\n",
        "\"\"\"\n",
        "\n",
        ".\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# 작업 디렉토리를 변경한다.\n",
        "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# Seq2Seq ChatBot : 학습 데이터 모듈\n",
        "# Google의 Sentencepiece를 이용해서 학습 데이터를 생성한다.\n",
        "#\n",
        "# 저작자: 2021.05.26, 조성현 (blog.naver.com/chunjein)\n",
        "# copyright: SNS 등에 공개할 때는 출처에 저작자를 명시해 주시기 바랍니다.\n",
        "# -------------------------------------------------------------------\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKNye4wzFue3"
      },
      "source": [
        "# 데이터 파일을 읽어온다.\n",
        "data_df = pd.read_csv('/content/drive/MyDrive/머신러닝/ChatBot/ChatBotData.csv', header=0)\n",
        "question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "# 특수 문자를 제거한다.\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "question = [re.sub(FILTERS, \"\", s) for s in question]\n",
        "answer = [re.sub(FILTERS, \"\", s) for s in answer]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTA0jlNhFzpC",
        "outputId": "8337eb99-375c-4a8a-a788-50624fdbffec"
      },
      "source": [
        "question[0], answer[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('12시 땡', '하루가 또 가네요')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcEnScLkF1FE",
        "outputId": "a2e4ee64-9873-4d4f-b199-a8c2fd1cee75"
      },
      "source": [
        "# Sentencepice용 사전을 만들기 위해 question + answer를 저장해 둔다.\n",
        "data_file = \"/content/drive/MyDrive/머신러닝/seq2seq/chatbot_data.txt\"\n",
        "with open(data_file, 'w', encoding='utf-8') as f:\n",
        "    for sent in question + answer:\n",
        "        f.write(sent + '\\n')\n",
        "        \n",
        "# Google의 Sentencepiece를 이용해서 vocabulary를 생성한다.\n",
        "# -----------------------------------------------------\n",
        "templates= \"--input={} \\\n",
        "            --pad_id=0 --pad_piece=<PAD>\\\n",
        "            --unk_id=1 --unk_piece=<UNK>\\\n",
        "            --bos_id=2 --bos_piece=<BOS>\\\n",
        "            --eos_id=3 --eos_piece=<EOS>\\\n",
        "            --model_prefix={} \\\n",
        "            --vocab_size={} \\\n",
        "            --character_coverage=1.0 \\\n",
        "            --model_type=unigram\"\n",
        "\n",
        "VOCAB_SIZE = 9000\n",
        "model_prefix = \"/content/drive/MyDrive/머신러닝/seq2seq/chatbot_model\"\n",
        "params = templates.format(data_file, model_prefix, VOCAB_SIZE)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(params)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(model_prefix + '.model')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OU0C2SdGogI"
      },
      "source": [
        "with open(model_prefix + '.vocab', encoding='utf-8') as f:\n",
        "    vocab = [doc.strip().split('\\t') for doc in f]\n",
        "\n",
        "word2idx = {k:v for v, [k, _] in enumerate(vocab)}\n",
        "idx2word = {v:k for v, [k, _] in enumerate(vocab)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d4zFapZGqqq"
      },
      "source": [
        "# 학습 데이터를 생성한다. (인코더 입력용, 디코더 입력용, 디코더 출력용)\n",
        "MAX_LEN = 15\n",
        "enc_input = []\n",
        "dec_input = []\n",
        "dec_output = []\n",
        "\n",
        "for Q, A in zip(question, answer):\n",
        "    # Encoder 입력\n",
        "    enc_i = sp.encode_as_ids(Q)\n",
        "    enc_input.append(enc_i)\n",
        "\n",
        "    # Decoder 입력, 출력\n",
        "    dec_i = [sp.bos_id()]   # <BOS>에서 시작함\n",
        "    dec_o = []\n",
        "    for ans in sp.encode_as_ids(A):\n",
        "        dec_i.append(ans)\n",
        "        dec_o.append(ans)\n",
        "    dec_o.append(sp.eos_id())   # Encoder 출력은 <EOS>로 끝남.        \n",
        "    \n",
        "    # dec_o는 <EOS>가 마지막에 들어있다. 나중에 pad_sequences()에서 <EOS>가\n",
        "    # 잘려 나가지 않도록 MAX_LEN 위치에 <EOS>를 넣어준다.\n",
        "    if len(dec_o) > MAX_LEN:\n",
        "        dec_o[MAX_LEN] = sp.eos_id()\n",
        "        \n",
        "    dec_input.append(dec_i)\n",
        "    dec_output.append(dec_o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-_VueTOGsTu"
      },
      "source": [
        "# 각 문장의 길이를 맞추고 남는 부분에 padding을 삽입한다.\n",
        "enc_input = pad_sequences(enc_input, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')\n",
        "dec_input = pad_sequences(dec_input, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')\n",
        "dec_output = pad_sequences(dec_output, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRBJ7qe8Gx2e",
        "outputId": "4de31778-d0e7-465b-d431-257a15e6d1e8"
      },
      "source": [
        "# 사전과 학습 데이터를 저장한다.\n",
        "with open('/content/drive/MyDrive/머신러닝/seq2seq/chatbot_voc.pkl', 'wb') as f:\n",
        "    pickle.dump([word2idx, idx2word], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/MyDrive/머신러닝/seq2seq/chatbot_train.pkl', 'wb') as f:\n",
        "    pickle.dump([enc_input, dec_input, dec_output], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "enc_input[0]\n",
        "\n",
        "dec_input[0]\n",
        "\n",
        "dec_output[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([259,   6, 100,  89,  36,   3,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmF3ebtfGzVW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}