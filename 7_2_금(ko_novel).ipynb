{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/2 금(ko_novel).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aXyGlTQ3RpDXpZ6lfCHCwKKUiq0cdgjI",
      "authorship_tag": "ABX9TyNIrYSg4IluET2QdFtHrBlb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_2_%EA%B8%88(ko_novel).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g0EuiDO-Dyi",
        "outputId": "cac885c9-e67b-46b0-a879-5c7784308a41"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHOaCKsa5k0h",
        "outputId": "f697f920-1f60-495b-b8a7-9d94216cfccc"
      },
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Okt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "f = open('/content/drive/MyDrive/머신러닝/ko_novel.txt', 'r')\n",
        "text = f.read()\n",
        "re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]*\", \"\",text)\n",
        "\n",
        "okt = Okt()\n",
        "sent_nj = []\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sentence in sentences:  \n",
        "  njv = [word for word, pos in okt.pos(sentence) if pos=='Adjective' or pos=='Noun']\n",
        "  sent_nj.append(njv)\n",
        "\n",
        "print(\"총 문장 개수 =\", len(sent_nj))\n",
        "print(sent_nj[0])\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sent_nj)\n",
        "\n",
        "# 단어사전\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "\n",
        "print(\"사전 크기 =\", len(word2idx))\n",
        "\n",
        "# 문장을 단어의 인덱스로 표현\n",
        "sent_idx = tokenizer.texts_to_sequences(sent_nj)\n",
        "sent_idx[0]\n",
        "\n",
        "def get_context(x, count = True):\n",
        "    idx = word2idx[x]\n",
        "    word_count = {v:0 for k,v in word2idx.items()}\n",
        "    for s_idx in sent_idx:\n",
        "        if idx in s_idx:\n",
        "            for i in s_idx:\n",
        "                word_count[i] += 1\n",
        "\n",
        "    result = sorted(word_count.items(), key=(lambda x: x[1]), reverse=True)\n",
        "    result = [(idx2word[i], c) for i, c in result[:30]]\n",
        "\n",
        "    if count:\n",
        "        return result\n",
        "    else:\n",
        "        return [w for w, c in result]\n",
        "\n",
        "mother = get_context('아버지', count=True)\n",
        "father = get_context('어머니', count=True)\n",
        "doctor = get_context('의사', count=True)\n",
        "\n",
        "def jaccard(x, y):\n",
        "    hab = set(x) | set(y)\n",
        "    gyo = set(x) & set(y)\n",
        "    return len(gyo) / len(hab)\n",
        "\n",
        "jaccard(mother, father)\n",
        "jaccard(mother, doctor)\n",
        "\n",
        "mother\n",
        "\n",
        "father\n",
        "\n",
        "doctor"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "총 문장 개수 = 55016\n",
            "['젊은', '느티나무', '강신재', '그', '비누', '냄새']\n",
            "사전 크기 = 33305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('의사', 87),\n",
              " ('것', 26),\n",
              " ('그', 21),\n",
              " ('나', 15),\n",
              " ('말', 15),\n",
              " ('사람', 8),\n",
              " ('수', 7),\n",
              " ('자기', 7),\n",
              " ('손', 6),\n",
              " ('내', 5),\n",
              " ('생각', 5),\n",
              " ('그것', 5),\n",
              " ('같은', 5),\n",
              " ('때문', 5),\n",
              " ('무슨', 5),\n",
              " ('자신', 5),\n",
              " ('아무', 5),\n",
              " ('있었다', 4),\n",
              " ('있는', 4),\n",
              " ('뒤', 4),\n",
              " ('아버지', 4),\n",
              " ('지금', 4),\n",
              " ('시어머니', 4),\n",
              " ('직업', 4),\n",
              " ('요한', 4),\n",
              " ('그런', 3),\n",
              " ('얼굴', 3),\n",
              " ('하나', 3),\n",
              " ('앞', 3),\n",
              " ('가지', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}