{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq(chat_attention).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1yAay3TQuJh625v4CPde1g1q5tLIbePHY",
      "authorship_tag": "ABX9TyNzii+xzdggD5YkthzX7SBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/seq2seq(chat_attention).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp_CUnWejf1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42dcc795-4a6a-47f4-e528-f01e07ed40c8"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"7-5.seq2seq(chat_attention).ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Kp1f7QjtFN3iTyNwOQDofxjcXkiA-05D\n",
        "\"\"\"\n",
        "\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 8.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUefB0o4e9Mb"
      },
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# 작업 디렉토리를 변경한다.\n",
        "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# Seq2Seq-Attention 모델를 이용한 ChatBot : 채팅 모듈\n",
        "#\n",
        "# 저작자: 2021.07.30, 조성현 (blog.naver.com/chunjein)\n",
        "# copyright: SNS 등에 공개할 때는 출처에 저작자를 명시해 주시기 바랍니다.\n",
        "# ----------------------------------------------------------------------\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dot, Concatenate\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZq0-YXe_xg"
      },
      "source": [
        "# Sub-word 사전 읽어온다.\n",
        "with open('/content/drive/MyDrive/머신러닝/seq2seq/chatbot_voc.pkl', 'rb') as f:\n",
        "    word2idx,  idx2word = pickle.load(f)\n",
        "\n",
        "# BLEU 평가를 위해 시험 데이터를 읽어온다.\n",
        "with open('/content/drive/MyDrive/머신러닝/seq2seq/chatbot_train.pkl', 'rb') as f:\n",
        "    _, _, _, que_test, ans_test = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9s8E5f5fJul"
      },
      "source": [
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "LSTM_HIDDEN = 128\n",
        "MAX_LEN = 15            # 단어 시퀀스 길이\n",
        "MODEL_PATH = '/content/drive/MyDrive/머신러닝/seq2seq/chatbot_trained.h5'\n",
        "\n",
        "# 데이터 전처리 과정에서 생성한 SentencePiece model을 불러온다.\n",
        "SPM_MODEL = \"/content/drive/MyDrive/머신러닝/seq2seq/chatbot_model.model\"\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(SPM_MODEL)\n",
        "\n",
        "# Encoder 출력과 decoder 출력으로 attention value를 생성하고,\n",
        "# decoder 출력 + attention value (concatenate)를 리턴한다.\n",
        "# x : encoder 출력, y : decoder 출력\n",
        "# LSTM time step = 4, SMB_SIZE = 3 이라면 각 텐서의 dimension은\n",
        "# 아래 주석과 같다.\n",
        "def Attention(x, y):\n",
        "    # step-1:\n",
        "    # decoder의 매 시점마다 encoder의 전체 시점과 dot-product을 수행한다.\n",
        "    score = Dot(axes=(2, 2))([y, x])                   # (1, 4, 4)\n",
        "    \n",
        "    # step-2:\n",
        "    # dot-product 결과를 확률분포로 만든다 (softmax)\n",
        "    # 이것이 attention score이다.\n",
        "    dist = Activation('softmax')(score)                # (1, 4, 4)\n",
        "\n",
        "    # step-3:\n",
        "    # Attention value를 계산한다.\n",
        "    attention = Dot(axes=(2, 1))([dist, x])\n",
        "\n",
        "    # step-4:\n",
        "    # decoder 출력과 attention을 concatenate 한다.\n",
        "    return Concatenate()([y, attention])    # (1, 4, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eep9d-6fHFH",
        "outputId": "da7f7851-1a6f-4664-b8d2-9011aabee9eb"
      },
      "source": [
        "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
        "K.clear_session()\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
        "\n",
        "# Encoder\n",
        "# -------\n",
        "encoderX = Input(batch_shape=(None, MAX_LEN))\n",
        "encEMB = wordEmbedding(encoderX)\n",
        "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "encLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
        "ey1, eh1, ec1 = encLSTM1(encEMB)      # LSTM 1층 \n",
        "ey2, eh2, ec2 = encLSTM2(ey1)         # LSTM 2층\n",
        "\n",
        "# Decoder\n",
        "# -------\n",
        "# Decoder는 1개 단어씩을 입력으로 받는다. 학습 때와 달리 문장 전체를 받아\n",
        "# recurrent하는 것이 아니라, 단어 1개씩 입력 받아서 다음 예상 단어를 확인한다.\n",
        "# chatting()에서 for 문으로 단어 별로 recurrent 시킨다.\n",
        "# 따라서 batch_shape = (None, 1)이다. 즉, time_step = 1이다. 그래도 네트워크\n",
        "# 파라메터는 동일하다.\n",
        "decoderX = Input(batch_shape=(None, 1))\n",
        "decEMB = wordEmbedding(decoderX)\n",
        "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
        "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
        "att_dy2 = Attention(ey2, dy2)\n",
        "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "outputY = decOutput(att_dy2)\n",
        "\n",
        "# Model\n",
        "# -----\n",
        "model = Model([encoderX, decoderX], outputY)\n",
        "#model.load_weights(MODEL_PATH)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           multiple             1152000     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 15, 128), (N 131584      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 1, 128), (No 131584      embedding[1][0]                  \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 15, 128), (N 131584      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 1, 128), (No 131584      lstm_2[0][0]                     \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 15)        0           lstm_3[0][0]                     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 1, 15)        0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 128)       0           activation[0][0]                 \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1, 256)       0           lstm_3[0][0]                     \n",
            "                                                                 dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 1, 9000)      2313000     concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 3,991,336\n",
            "Trainable params: 3,991,336\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfpeOWPBfF2B",
        "outputId": "df74105e-d808-4d60-cac1-642124c389dd"
      },
      "source": [
        "# Chatting용 model\n",
        "model_enc = Model(encoderX, [eh1, ec1, eh2, ec2, ey2])\n",
        "\n",
        "ih1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ih2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ic2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
        "ey = Input(batch_shape = (None, MAX_LEN, LSTM_HIDDEN))\n",
        "\n",
        "dec_output1, dh1, dc1 = decLSTM1(decEMB, initial_state = [ih1, ic1])\n",
        "dec_output2, dh2, dc2 = decLSTM2(dec_output1, initial_state = [ih2, ic2])\n",
        "dec_attention = Attention(ey, dec_output2)\n",
        "dec_output = decOutput(dec_attention)\n",
        "model_dec = Model([decoderX, ih1, ic1, ih2, ic2, ey], \n",
        "                  [dec_output, dh1, dc1, dh2, dc2])\n",
        "model_dec.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           multiple             1152000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 1, 128), (No 131584      embedding[1][0]                  \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 1, 128), (No 131584      lstm_2[1][0]                     \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 15, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, 1, 15)        0           lstm_3[1][0]                     \n",
            "                                                                 input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1, 15)        0           dot_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 1, 128)       0           activation_1[0][0]               \n",
            "                                                                 input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1, 256)       0           lstm_3[1][0]                     \n",
            "                                                                 dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 1, 9000)      2313000     concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 3,728,168\n",
            "Trainable params: 3,728,168\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAa-2H16fDSl"
      },
      "source": [
        "# Question을 입력받아 Answer를 생성한다.\n",
        "def genAnswer(question):\n",
        "    question = question[np.newaxis, :]\n",
        "    init_h1, init_c1, init_h2, init_c2, enc_y = model_enc.predict(question)\n",
        "\n",
        "    # 시작 단어는 <BOS>로 한다.\n",
        "    word = np.array(sp.bos_id()).reshape(1, 1)\n",
        "\n",
        "    answer = []\n",
        "    for i in range(MAX_LEN):\n",
        "        dY, next_h1, next_c1, next_h2, next_c2 = \\\n",
        "            model_dec.predict([word, init_h1, init_c1, init_h2, init_c2, enc_y])\n",
        "        \n",
        "        # 디코더의 출력은 vocabulary에 대응되는 one-hot이다.\n",
        "        # argmax로 해당 단어를 채택한다.\n",
        "        nextWord = np.argmax(dY[0, 0])\n",
        "\n",
        "        # 예상 단어가 <EOS>이거나 <PAD>이면 더 이상 예상할 게 없다.\n",
        "        if nextWord == sp.eos_id() or nextWord == sp.pad_id():\n",
        "            break\n",
        "        \n",
        "        # 다음 예상 단어인 디코더의 출력을 answer에 추가한다.\n",
        "        answer.append(idx2word[nextWord])\n",
        "        \n",
        "        # 디코더의 다음 recurrent를 위해 입력 데이터와 hidden 값을\n",
        "        # 준비한다. 입력은 word이고, hidden은 h와 c이다.\n",
        "        word = np.array(nextWord).reshape(1,1)\n",
        "    \n",
        "        init_h1 = next_h1\n",
        "        init_c1 = next_c1\n",
        "        init_h2 = next_h2\n",
        "        init_c2 = next_c2\n",
        "        \n",
        "    return sp.decode_pieces(answer)\n",
        "\n",
        "def make_question(que_string):\n",
        "    q_idx = []\n",
        "    for x in sp.encode_as_pieces(que_string):\n",
        "        if x in word2idx:\n",
        "            q_idx.append(word2idx[x])\n",
        "        else:\n",
        "            q_idx.append(sp.unk_id())   # out-of-vocabulary (OOV)\n",
        "    \n",
        "    # <PAD>를 삽입한다.\n",
        "    if len(q_idx) < MAX_LEN:\n",
        "        q_idx.extend([sp.pad_id()] * (MAX_LEN - len(q_idx)))\n",
        "    else:\n",
        "        q_idx = q_idx[0:MAX_LEN]\n",
        "    return q_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL4ZWFlvfBe4",
        "outputId": "b2ce3eeb-0411-4562-9522-e4ce7aa8e1da"
      },
      "source": [
        "# BLEU 평가\n",
        "bleu_list = []\n",
        "for que_str, reference in zip(que_test[:10], ans_test[:10]):\n",
        "    q_idx = make_question(que_str)\n",
        "    candidate = genAnswer(np.array(q_idx))\n",
        "\n",
        "    # BLEU를 측정한다.\n",
        "    # 기계번역의 reference는 어느정도 객관성이 있지만, 일상 대화용 챗봇의 reference는 매우 주관적이기 때문에,\n",
        "    # test data로 측정한 챗봇의 BLEU는 매우 낮을 수밖에 없다. 특정 업무를 위한 챗봇의 reference는 어느정도\n",
        "    # 객관성이 있을 수 있다.\n",
        "    #\n",
        "    # 1. 짧은 문장이 많기 때문에 단어가 아닌 subword 단위로 BLEU를 측정한다.\n",
        "    # 2. (Papineni et al. 2002)은 micro-average를 사용했지만, 여기서는 단순 평균인 macro-average를 사용한다.\n",
        "    reference = sp.encode_as_pieces(reference)\n",
        "    candidate = sp.encode_as_pieces(candidate)\n",
        "\n",
        "    bleu = sentence_bleu([reference], candidate, weights=[1/2., 1/2.])\n",
        "    bleu_list.append(bleu)\n",
        "    print(que_str, '-->', sp.decode_pieces(candidate), ':', np.round(bleu, 4))\n",
        "print('Average BLEU score =', np.round(np.mean(bleu_list), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "좋아하는 애랑 영화 볼건데 액션은 좀 그렇겠지 --> 되가는데되가는데별별별추적추적 반지 반지 반지 반지 든 든 든 든 : 0.2949\n",
            "썸을 오래 탔는데 사귀어도 되는 거야 --> 뻣뻣 편안하 편안하 짝사랑 짝사랑 식도염 식도염 식도염 식도염 속지 속지 속지 낮술 생각나네 : 0.1925\n",
            "너무 힘들다 지쳤어 --> 이용당 짝사랑 짝사랑 짝사랑안하안하 의지해 의지해 의지해 모으 죽겠 죽겠 졸 졸 졸 : 0\n",
            "에궁 --> 이용당 나아질 짝사랑 두고 두고 목 목 목 쌍커풀 쌍커풀 쌍커풀 쌍커풀 쌍커풀 쌍커풀 쌍커풀 : 0\n",
            "중국으로 진출해볼까 --> 불공평해 짝사랑안하안하안하 목 목 의지해 의지해 의지해 죽겠 죽겠 죽겠 홈옮 : 0\n",
            "사귀고 나서 트레이닝복만 입어 --> 불공평해료자신자신자신 가라앉 가라앉다녀와 자 자 귀여운 개당황 자 간다 다되어가네 : 0\n",
            "혹시 그런 날 없나 --> 이용당 짝사랑 짝사랑안하안하 의지해 일인 일인 의지해 쌍커풀 쌍커풀 쌍커풀 쌍커풀 쌍커풀 쌍커풀 : 0\n",
            "오늘 헤어졌어여 --> 이용당 나아질 짝사랑 망 망 망 부작용 부작용 부작용 부작용 부작용 모쏠개월빙빙 : 0\n",
            "너무나도 힘들어 --> 이용당 짝사랑 짝사랑 짝사랑안하 뿌 뿌 의지해 의지해 의지해 조급하 조급하 그럼요 나아질 나아질 : 0\n",
            "야경 보러 가고 싶다 --> 이용당 짝사랑 짝사랑 짝사랑 두고 두고 목 진행 못하겠 목겠지겠지 싫어해 경험해보 경험해보 : 0\n",
            "Average BLEU score = 0.0487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxWiWViJfAYm"
      },
      "source": [
        "# Chatting\n",
        "# dummy : 최초 1회는 모델을 로드하는데 약간의 시간이 걸리므로 이것을 가리기 위함.\n",
        "def chatting(n=100):\n",
        "    for i in range(n):\n",
        "        question = input('Q : ')\n",
        "        \n",
        "        if  question == 'quit':\n",
        "            break\n",
        "        \n",
        "        q_idx = make_question(question)\n",
        "        answer = genAnswer(np.array(q_idx))\n",
        "        print('A :', answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDn2al57f1MN",
        "outputId": "adec1553-d21b-4eba-b958-4f39a2334373"
      },
      "source": [
        "####### Chatting 시작 #######\n",
        "print(\"\\nSeq2Seq ChatBot (ver. 1.0)\")\n",
        "print(\"Chatting 모듈을 로드하고 있습니다 ...\")\n",
        "\n",
        "# 처음 1회는 시간이 걸리기 때문에 dummy question을 입력한다.\n",
        "answer = genAnswer(np.zeros(MAX_LEN))\n",
        "print(\"ChatBot이 준비 됐습니다.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Seq2Seq ChatBot (ver. 1.0)\n",
            "Chatting 모듈을 로드하고 있습니다 ...\n",
            "ChatBot이 준비 됐습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOyd2AJsf2wv",
        "outputId": "000d42c7-40be-4eb1-a6ae-b54f0de971e8"
      },
      "source": [
        "# 채팅을 시작한다.\n",
        "chatting(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q : 이별 후 1년 그리고 선물\n",
            "A : 장점 짝사랑안하안하안하 목 목 목 돌아올 돌아올받아받아받아▁모쏠▁모쏠\n",
            "Q : 허기져\n",
            "A : 나아질 나아질 짝사랑 두고 두고 두고 두고 죽겠 죽겠 죽겠 못하겠흑▁경험해보▁경험해보▁경험해보\n",
            "Q : 맥주 소주 어떤거 마실까\n",
            "A : 잡굳굳굳몰 목 목 목▁자유시간뱄겠지▁일인▁일인 생긴뱄\n",
            "Q : 교양 수업 재밌어\n",
            "A : ▁이용당 짝사랑 짝사랑 짝사랑 두고 두고 진행 못하겠 못하겠 남자가 남자가▁쌍커풀▁쌍커풀▁쌍커풀▁쌍커풀\n",
            "Q : 권태기 이별\n",
            "A : 나아질 짝사랑 짝사랑 두고 두고 두고 목 목 죽겠▁끌 메모 메모▁일인▁일인미칠꺼\n",
            "Q : 읽씹은 아프네\n",
            "A : 잡 괜찮다 짝사랑 짝사랑안하안하안하 목 의지해 오네 모으 모으받아받아킨\n",
            "Q : quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfdCl1gDf3LD"
      },
      "source": [
        "# train data:\n",
        "# [('이별 후 1년 그리고 선물', '이별하신게 맞나요'),\n",
        "#  ('허기져', '챙겨 드세요'),\n",
        "#  ('맥주 소주 어떤거 마실까', '소맥이요'),\n",
        "#  ('교양 수업 재밌어', '저도 듣고 싶어요'),\n",
        "#  ('권태기 이별', '극복하거나 이별하거나 둘 중 하나죠'),\n",
        "#  ('읽씹은 아프네', '상대방에 대한 예의가 없네요'),\n",
        "#  ('신혼여행 어디로 갈까', '못 가본 곳으로 가보세요'),\n",
        "#  ('반 배정 잘 될까', '잘 되길 바랍니다'),\n",
        "#  ('친구가 다 떠나서 내가 못났나 싶어', '지난 인연에 연연해하지 마세요'),\n",
        "#  ('뒤돌아 보지 말고 나가야 하는데', '살짝 뒤돌아봐도 괜찮아요')]\n",
        "\n",
        "# test data:\n",
        "# [('소오름 쫙', '좋은 일이길 바랍니다'),\n",
        "#  ('고백은 어떻게 하는거야', '솔직한 마음으로 다가가는 거죠'),\n",
        "#  ('참 잘낫네', '진정하셔요'),\n",
        "#  ('늘 빡빡하게 살기 힘드네', '여유가 생기길 바랍니다'),\n",
        "#  ('집까지 데려다줬는데 호감 그냥 매너', '호감이 있을 수도 있어요 그렇지만 조금 더 상황을 지켜보세요'),\n",
        "#  ('짝녀가 연락 안 되고 있는데 자나', '자고 있을지도 모르겠어요'),\n",
        "#  ('마음도 춥고 날씨도 춥고', '마음 감기 조심하세요'),\n",
        "#  ('죽었던 연애세포가 살아나는 것 같아', '좋은 소식이네요'),\n",
        "#  ('겨울에는 온천이지', '몸은 뜨겁고 머리는 차갑게'),\n",
        "#  ('소개팅 하고싶다', '친구한테 부탁해보세요')]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
