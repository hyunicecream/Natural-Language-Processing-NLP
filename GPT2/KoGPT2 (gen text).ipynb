{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-12.KoGPT2(gen_text).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1L_-3JvHUtgbNBQSR5srUhMpg-WcybNGl",
      "authorship_tag": "ABX9TyM6kkrZdhKXwIMdCbOyuwkv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/KoGPT2(gen_text).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBAAKsrIBqfL",
        "outputId": "8e8ce612-b5d9-4f24-e7b6-6afdeec18c46"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug 11 02:41:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_bM5cU5Eglk",
        "outputId": "d41529bc-3445-4e51-f5ef-af236312b792"
      },
      "source": [
        "!pip install --upgrade mxnet>=1.6.0\n",
        "!pip install gluonnlp\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.23)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNQjDQmbBO2X"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"8-12.KoGPT2(gen_text).ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ABZelGJI7VxTiwsnmD6Fy3o85hxXfmw0\n",
        "\"\"\"\n",
        "\n",
        "# GPT2의 TFGPT2LMHeadModel 모델을 이용한 언어 생성\n",
        "# TFGPT2LMHeadModel : The GPT2 Model transformer with a language modeling head on top \n",
        "#                     (linear layer with weights tied to the input embeddings).\n",
        "# !pip install --upgrade mxnet>=1.6.0\n",
        "# !pip install gluonnlp\n",
        "# !pip install transformers\n",
        "# !pip install sentencepiece\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "import tensorflow as tf"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GKMA1q2BnHp"
      },
      "source": [
        "MY_PATH = '/content/drive/MyDrive/머신러닝/GPT/'\n",
        "MODEL_PATH = MY_PATH + 'gpt_ckpt'\n",
        "TOKENIZER_PATH = MY_PATH + 'gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "detokenizer = SentencepieceDetokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token = None,\n",
        "                                               sep_token = None,\n",
        "                                               cls_token = None,\n",
        "                                               unknown_token = '<unk>',\n",
        "                                               padding_token = '<pad>',\n",
        "                                               bos_token = '<s>',\n",
        "                                               eos_token = '</s>')\n",
        "# vocab --> Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "DIt7pYSJBkzT",
        "outputId": "2f278a7c-0c45-428c-fd99-32adb2834fd0"
      },
      "source": [
        "# tokenizer 연습\n",
        "# 참고 : https://nlp.gluon.ai/api/modules/data.html\n",
        "toked = tokenizer('안녕 하세요')   # tokenizer가 잘못 만들어진 듯 ... 한글자씩 ?\n",
        "print(toked)\n",
        "\n",
        "toked_idx = vocab(toked)\n",
        "print(toked_idx)\n",
        "\n",
        "toked = vocab.to_tokens(toked_idx)\n",
        "print(toked)\n",
        "\n",
        "detoked = detokenizer(toked)\n",
        "print(detoked)\n",
        "\n",
        "''.join(toked).replace('▁', ' ')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', '안', '녕', '▁', '하', '세', '요']\n",
            "[47437, 47541, 48439, 47437, 47451, 47530, 47580]\n",
            "['▁', '안', '녕', '▁', '하', '세', '요']\n",
            "안녕 하세요\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' 안녕 하세요'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfrW1sckBkK6",
        "outputId": "4c5cbce8-75ea-4154-a1aa-b8d23083d922"
      },
      "source": [
        "model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/MyDrive/머신러닝/GPT/gpt_ckpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tfgp_t2lm_head_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "transformer (TFGPT2MainLayer multiple                  124242432 \n",
            "=================================================================\n",
            "Total params: 124,242,432\n",
            "Trainable params: 124,242,432\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FS2aY_rBiuA"
      },
      "source": [
        "# 모델의 seed 입력 문장 생성\n",
        "tok = tokenizer('이때')   # tok = ['▁', '이', '때']\n",
        "tok_idx = [vocab[vocab.bos_token]] + vocab[tok]     # tok_idx = [0, 47437, 47438, 47675]\n",
        "input_ids = tf.convert_to_tensor(tok_idx)[None, :]  # 텐서로 변환"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNGt0eJ8BigB",
        "outputId": "817b08f9-cef7-4ba0-eecf-28a342e4adcc"
      },
      "source": [
        "# 모델의 출력\n",
        "output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "output"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 50), dtype=int32, numpy=\n",
              "array([[    0, 47437, 47438, 47675,   211,  3918,   164,   831,   638,\n",
              "          148,   222, 47929, 47454,   164,   831,   638,   222, 47929,\n",
              "        47454,   164,   831,   638,   222, 47929, 47454,   164,   831,\n",
              "          638,   222, 47929, 47454,   164,   831,   638,   222, 47929,\n",
              "        47454,   164,   831,   638,   222, 47929, 47454,   164,   831,\n",
              "          638,   222, 47929, 47454,   164]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pHgdUQLBh_-",
        "outputId": "7427450f-c9c5-4352-9ed1-85a40b6fc8c4"
      },
      "source": [
        "# 모델의 출력을 문자열로 변환\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이때부터 나는 내 모든 것을 다 바쳐서 내 모든 것을 바쳐서 내 모든 것을 바쳐서 내 모든 것을 바쳐서 내 모든 것을 바쳐서 내 모든 것을 바쳐서 내 모든 것을 바쳐서 내\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdHTQZfQBhpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055a2e0d-1ce9-4f24-f64b-2ad879084301"
      },
      "source": [
        "# Beam search\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "이때부터  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지  ⁇ 그제까지\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh6sfjJ3BhOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0ad378-a752-424d-ecc1-11e27071cc19"
      },
      "source": [
        "# 연속된 단어가 나오는 것을 방지함. no_repeat_ngram_size = 2\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "이때쯤이면  ⁇ 그제 같을 것 같다. \"어머나, 이 녀석!!!!!!!\" ^^* Subject 2말괄량이프린세스 9장 *063* 로맨스\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut3JCTyNBgs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e803cbd-c95a-4503-d4d5-62b61b8dbae6"
      },
      "source": [
        "# top_k sampling\n",
        "output = model.generate(input_ids, max_length=50, do_sample = True, top_k=100, temperature=0.8)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "이때야말로 그런 생각이 간절했던 것도 같다. 또 다른 전문가들은 정부가 고용시장 변화를 살피지 않고 무작정 밀어붙이기만을 할 경우 '고용 없는 성장'을 가져온다는 점을 강조할 것으로 보인다. 전문가들은 \"미국의 경기 회복\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOrohaaxBgba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90773af1-f81b-4898-cc4c-fe7d87e13871"
      },
      "source": [
        "# top_p sampling\n",
        "output = model.generate(input_ids, max_length=50, do_sample = True, top_p=0.9, temperature=0.8)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "이때부터 사귄 건 아니고 내가 원하는 걸 다 받아 줄 테니깐 그때는 내가 원하는 게 뭐든 다 받아 줄께 자? 미안해 자? 잘 자 자?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUwrFqzRBgKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681d2947-df2c-4448-b3d3-cb8ead675de1"
      },
      "source": [
        "# top_k & top_p sampling\n",
        "output = model.generate(input_ids, max_length=50, do_sample = True, top_k=100, top_p=0.9, temperature=0.8)\n",
        "\n",
        "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
        "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
        "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
        "print(out_text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "이때부터 미류는 미류의 마음을 알아채고 있었다. \"미류, 왜 그래? 이제 그만 자...  ⁇ , 그 시간에 잠자구 있잖아.  ⁇ , 그 시간에\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssIwEJoIGj5J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
