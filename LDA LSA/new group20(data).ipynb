{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7/22  목-new_group20(data).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Y9_zUiKKv6Am2baikLy1FCsutEb6vmi_",
      "authorship_tag": "ABX9TyNDv+e091DAl7chF1/EYTzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/7_22_%EB%AA%A9_new_group20(data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjBHQuUGyyOZ",
        "outputId": "17bd531b-f892-421f-f103-48a9b5224e00"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"news_group20(data).ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1XD47NnjO24x5atsXYp8S8vYryOq69b-m\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# %cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# news data를 읽어와서 저장해 둔다.\n",
        "newsData = fetch_20newsgroups(shuffle=True, \n",
        "                             random_state=1, remove=('fotters', 'quotes'))\n",
        "                             #remove=('headers', 'footers', 'quotes'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QraXWdvey_TP",
        "outputId": "1ba0f6a1-889e-4135-abc6-05ccd2d25b2b"
      },
      "source": [
        "# 첫 번째 news를 조회해 본다.\n",
        "news = newsData['data']\n",
        "topic = newsData['target']\n",
        "topic_name = newsData['target_names']\n",
        "\n",
        "print(len(news))\n",
        "print(news[0])\n",
        "print('topic = ', topic[0], topic_name[topic[0]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314\n",
            "From: ab4z@Virginia.EDU (\"Andi Beyer\")\n",
            "Subject: Re: Israeli Terrorism\n",
            "Organization: University of Virginia\n",
            "Lines: 15\n",
            "\n",
            "Well i'm not sure about the story nad it did seem biased. What\n",
            "I disagree with is your statement that the U.S. Media is out to\n",
            "ruin Israels reputation. That is rediculous. The U.S. media is\n",
            "the most pro-israeli media in the world. Having lived in Europe\n",
            "I realize that incidences such as the one described in the\n",
            "letter have occured. The U.S. media as a whole seem to try to\n",
            "ignore them. The U.S. is subsidizing Israels existance and the\n",
            "Europeans are not (at least not to the same degree). So I think\n",
            "that might be a reason they report more clearly on the\n",
            "atrocities.\n",
            "\tWhat is a shame is that in Austria, daily reports of\n",
            "the inhuman acts commited by Israeli soldiers and the blessing\n",
            "received from the Government makes some of the Holocaust guilt\n",
            "go away. After all, look how the Jews are treating other races\n",
            "when they got power. It is unfortunate.\n",
            "\n",
            "topic =  17 talk.politics.mideast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOtDYfJHzAHm",
        "outputId": "f6362a00-e459-452d-8e5f-84882ec82428"
      },
      "source": [
        "# news 별로 분류된 target을 확인해 본다.\n",
        "print(newsData['target_names'])\n",
        "print(newsData.target_names)\n",
        "print(len(newsData.target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNuXcVXtzAoo",
        "outputId": "969f383f-a1ee-4ba0-8ace-8990d5ae481e"
      },
      "source": [
        "# preprocessing.\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# 1. 영문자가 아닌 문자를 모두 제거한다.\n",
        "news1 = []\n",
        "for doc in news:\n",
        "    news1.append(re.sub(\"[^a-zA-Z]\", \" \", doc))\n",
        "\n",
        "# 2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 \n",
        "# 단어를 제거한다\n",
        "# 3. Porterstemmer를 적용한다.\n",
        "stop_words = stopwords.words('english')\n",
        "news2 = []\n",
        "for doc in news1:\n",
        "    doc1 = []\n",
        "    for w in doc.split():\n",
        "        w = w.lower()\n",
        "        if len(w) > 3 and w not in stop_words:\n",
        "            doc1.append(stemmer.stem(w))\n",
        "    news2.append(' '.join(doc1))\n",
        "\n",
        "# 전처리가 완료된 데이터를 저장한다.\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/pickle/newsgroup20.pkl', 'wb') as f:\n",
        "    pickle.dump([news2, newsData.target], f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "news2[1]\n",
        "\n",
        "newsData.target\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17,  0, 17, ...,  9,  4,  9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    }
  ]
}
