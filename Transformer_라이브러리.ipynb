{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer-라이브러리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoFfJjk5fKO5WzfIr7Mi6I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/Natural-Language-Processing-NLP-/blob/main/Transformer_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trmKqnEe3GdO"
      },
      "source": [
        "# https://github.com/suyash/transformer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding\n",
        "from tensorflow.keras.layers import Layer, LayerNormalization\n",
        "from tensorflow.keras.layers import Permute, Reshape\n",
        "\n",
        "class Transformer:\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 input_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 dropout_rate,\n",
        "                 ffn_activation=tf.keras.activations.relu,\n",
        "                 scope=\"transformer\"):\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               d_model=d_model,\n",
        "                               num_heads=num_heads,\n",
        "                               d_ff=d_ff,\n",
        "                               vocab_size=input_vocab_size,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               ffn_activation=ffn_activation,\n",
        "                               scope=\"%s/encoder\" % scope)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers,\n",
        "                               d_model=d_model,\n",
        "                               num_heads=num_heads,\n",
        "                               d_ff=d_ff,\n",
        "                               vocab_size=target_vocab_size,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               ffn_activation=ffn_activation,\n",
        "                               scope=\"%s/decoder\" % scope)\n",
        "\n",
        "        self.final_layer = Dense(target_vocab_size,\n",
        "                                 activation='softmax',\n",
        "                                 name=\"%s/dense\" % scope)\n",
        "\n",
        "        self.padding_mask = PaddingMask(name=\"%s/padding_mask\" % scope)\n",
        "        self.lookahead_mask = PaddingAndLookaheadMask(\n",
        "            name=\"%s/lookahead_mask\" % scope)\n",
        "\n",
        "    def __call__(self, inputs, target):\n",
        "        padding_mask = self.padding_mask(inputs)\n",
        "        lookahead_mask = self.lookahead_mask(target)\n",
        "\n",
        "        enc_output, enc_attention = self.encoder(inputs, padding_mask)\n",
        "\n",
        "        dec_output, dec_attention, enc_dec_attention = self.decoder(\n",
        "            target, enc_output, lookahead_mask, padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, enc_attention, dec_attention, enc_dec_attention\n",
        "\n",
        "class Decoder:\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 vocab_size,\n",
        "                 dropout_rate,\n",
        "                 ffn_activation=tf.keras.activations.relu,\n",
        "                 scope=\"decoder\"):\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.scope = scope\n",
        "\n",
        "        self.embedding = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=d_model,\n",
        "                                   name=\"%s/embedding\" % scope)\n",
        "        self.pos_encoding = PositionalEncoding(d_model,\n",
        "                                               name=\"%s/positional_encoding\" %\n",
        "                                               scope)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model=d_model,\n",
        "                         num_heads=num_heads,\n",
        "                         d_ff=d_ff,\n",
        "                         dropout_rate=dropout_rate,\n",
        "                         ffn_activation=ffn_activation,\n",
        "                         scope=\"%s/decoder_layer_%d\" % (scope, i))\n",
        "            for i in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate, name=\"%s/dropout\" % self.scope)\n",
        "\n",
        "    def __call__(self, x, enc_output, lookahead_mask, padding_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = MultiplyConstant(self.d_model, name=\"%s/multiply\" % self.scope)(x)\n",
        "        x = Add(name=\"%s/add\" % self.scope)([x, self.pos_encoding(x)])\n",
        "\n",
        "        dec_attention_weights = {}\n",
        "        enc_dec_attention_weights = {}\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, dec_attention, enc_dec_attention = self.dec_layers[i](\n",
        "                x, enc_output, lookahead_mask, padding_mask)\n",
        "\n",
        "            dec_attention_weights[\"layer_%d\" % i] = dec_attention\n",
        "            enc_dec_attention_weights[\"layer_%d\" % i] = enc_dec_attention\n",
        "\n",
        "        return x, dec_attention_weights, enc_dec_attention_weights\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 vocab_size,\n",
        "                 dropout_rate,\n",
        "                 ffn_activation=tf.keras.activations.relu,\n",
        "                 scope=\"encoder\"):\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.scope = scope\n",
        "\n",
        "        self.embedding = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=d_model,\n",
        "                                   name=\"%s/embedding\" % scope)\n",
        "        self.pos_encoding = PositionalEncoding(d_model,\n",
        "                                               name=\"%s/positional_encoding\" %\n",
        "                                               scope)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model=d_model,\n",
        "                         num_heads=num_heads,\n",
        "                         d_ff=d_ff,\n",
        "                         dropout_rate=dropout_rate,\n",
        "                         ffn_activation=ffn_activation,\n",
        "                         scope=\"%s/encoder_layer_%d\" % (scope, i))\n",
        "            for i in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate, name=\"%s/dropout\" % self.scope)\n",
        "\n",
        "    def __call__(self, x, padding_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = MultiplyConstant(self.d_model, name=\"%s/multiply\" % self.scope)(x)\n",
        "        x = Add(name=\"%s/add\" % self.scope)([x, self.pos_encoding(x)])\n",
        "\n",
        "        enc_attention_weights = {}\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, enc_attention = self.enc_layers[i](x, padding_mask)\n",
        "            enc_attention_weights[\"layer_%d\" % i] = enc_attention\n",
        "\n",
        "        return x, enc_attention_weights\n",
        "\n",
        "\n",
        "class DecoderLayer:\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 dropout_rate,\n",
        "                 ffn_activation=tf.keras.activations.relu,\n",
        "                 scope=\"decoder_layer\"):\n",
        "        self.scope = scope\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,\n",
        "                                       num_heads,\n",
        "                                       scope=\"%s/multi_head_attention_1\" %\n",
        "                                       scope)\n",
        "        self.mha2 = MultiHeadAttention(d_model,\n",
        "                                       num_heads,\n",
        "                                       scope=\"%s/multi_head_attention_2\" %\n",
        "                                       scope)\n",
        "        self.ffn = PointwiseFeedForwardNetwork(\n",
        "            d_model,\n",
        "            d_ff,\n",
        "            activation=ffn_activation,\n",
        "            scope=\"%s/pointwise_feed_forward_network\" % scope)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_1\" % scope)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_2\" % scope)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_3\" % scope)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate, name=\"%s/dropout_1\" % scope)\n",
        "        self.dropout2 = Dropout(dropout_rate, name=\"%s/dropout_2\" % scope)\n",
        "        self.dropout3 = Dropout(dropout_rate, name=\"%s/dropout_3\" % scope)\n",
        "\n",
        "    def __call__(self, x, enc_output, lookahead_mask, padding_mask):\n",
        "        out1, dec_dec_attention = self.mha1(x, x, x, lookahead_mask)\n",
        "        out1 = self.dropout1(out1)\n",
        "        x = Add(name=\"%s/add_1\" % self.scope)([x, out1])\n",
        "        x = self.layernorm1(x)\n",
        "\n",
        "        out2, enc_dec_attention = self.mha2(x, enc_output, enc_output,\n",
        "                                            padding_mask)\n",
        "        out2 = self.dropout2(out2)\n",
        "        x = Add(name=\"%s/add_2\" % self.scope)([x, out2])\n",
        "        x = self.layernorm2(x)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        x = Add(name=\"%s/add_3\" % self.scope)([x, ffn_output])\n",
        "        x = self.layernorm3(x)\n",
        "\n",
        "        return x, dec_dec_attention, enc_dec_attention\n",
        "\n",
        "class EncoderLayer:\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 dropout_rate,\n",
        "                 ffn_activation=tf.keras.activations.relu,\n",
        "                 scope=\"encoder_layer\"):\n",
        "        self.scope = scope\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,\n",
        "                                       num_heads,\n",
        "                                       scope=\"%s/multi_head_attention_1\" %\n",
        "                                       scope)\n",
        "        self.ffn = PointwiseFeedForwardNetwork(\n",
        "            d_model,\n",
        "            d_ff,\n",
        "            activation=ffn_activation,\n",
        "            scope=\"%s/pointwise_feed_forward_network\" % scope)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_1\" % scope)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_2\" % scope)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate, name=\"%s/dropout_1\" % scope)\n",
        "        self.dropout2 = Dropout(dropout_rate, name=\"%s/dropout_2\" % scope)\n",
        "\n",
        "    def __call__(self, x, padding_mask):\n",
        "        out1, enc_enc_attention = self.mha1(x, x, x, padding_mask)\n",
        "        out1 = self.dropout1(out1)\n",
        "        x = Add(name=\"%s/add_1\" % self.scope)([x, out1])\n",
        "        x = self.layernorm1(x)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        x = Add(name=\"%s/add_2\" % self.scope)([x, ffn_output])\n",
        "        x = self.layernorm2(x)\n",
        "\n",
        "        return x, enc_enc_attention\n",
        "\n",
        "\n",
        "class PointwiseFeedForwardNetwork:\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 d_ff,\n",
        "                 activation=tf.keras.activations.relu,\n",
        "                 scope=\"pointwise_feed_forward_network\"):\n",
        "        self.dense_1 = Dense(d_ff,\n",
        "                             activation=activation,\n",
        "                             name=\"%s/dense_1\" % scope)\n",
        "        self.dense_2 = Dense(d_model,\n",
        "                             activation=None,\n",
        "                             name=\"%s/dense_2\" % scope)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.dense_2(self.dense_1(x))\n",
        "\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads, scope=\"multi_head_attention\"):\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.wq = Dense(d_model, name=\"%s/dense_q\" % scope)\n",
        "        self.wk = Dense(d_model, name=\"%s/dense_k\" % scope)\n",
        "        self.wv = Dense(d_model, name=\"%s/dense_v\" % scope)\n",
        "\n",
        "        self.reshapeq = Reshape((-1, num_heads, d_model // num_heads),\n",
        "                                name=\"%s/reshape_q\" % scope)\n",
        "        self.reshapek = Reshape((-1, num_heads, d_model // num_heads),\n",
        "                                name=\"%s/reshape_k\" % scope)\n",
        "        self.reshapev = Reshape((-1, num_heads, d_model // num_heads),\n",
        "                                name=\"%s/reshape_v\" % scope)\n",
        "\n",
        "        self.transposeq = Permute((2, 1, 3), name=\"%s/transpose_q\" % scope)\n",
        "        self.transposek = Permute((2, 1, 3), name=\"%s/transpose_k\" % scope)\n",
        "        self.transposev = Permute((2, 1, 3), name=\"%s/transpose_v\" % scope)\n",
        "\n",
        "        self.reshape_output = Reshape((-1, d_model),\n",
        "                                      name=\"%s/reshape_output\" % scope)\n",
        "\n",
        "        self.transpose_output = Permute((2, 1, 3),\n",
        "                                        name=\"%s/transpose_output\" % scope)\n",
        "\n",
        "        self.dense = Dense(d_model, name=\"%s/dense\" % scope)\n",
        "\n",
        "        self.attention = Attention(name=\"%s/attention\" % scope)\n",
        "\n",
        "    def __call__(self, q, k, v, mask):\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.reshapeq(q)\n",
        "        k = self.reshapek(k)\n",
        "        v = self.reshapev(v)\n",
        "\n",
        "        q = self.transposeq(q)\n",
        "        k = self.transposek(k)\n",
        "        v = self.transposev(v)\n",
        "\n",
        "        x, attention_weights = self.attention([q, k, v, mask])\n",
        "\n",
        "        x = self.transpose_output(x)\n",
        "        x = self.reshape_output(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __call__(self, inputs):\n",
        "        q, k, v, mask = inputs\n",
        "\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        scaled_attention_logits += mask * -1e9\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, d_model, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        position = tf.shape(inputs)[1]\n",
        "\n",
        "        position_dims = tf.range(position)[:, tf.newaxis]\n",
        "        embed_dims = tf.range(self.d_model)[tf.newaxis, :]\n",
        "        angle_rates = 1 / tf.pow(\n",
        "            10000.0, tf.cast(\n",
        "                (2 * (embed_dims // 2)) / self.d_model, tf.float32))\n",
        "        angle_rads = tf.cast(position_dims, tf.float32) * angle_rates\n",
        "\n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return dict(list(base.items()) + [(\"d_model\", self.d_model)])\n",
        "\n",
        "\n",
        "class MultiplyConstant(Layer):\n",
        "    def __init__(self, c, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.c = c\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return inputs * self.c\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return dict(list(base.items()) + [(\"c\", self.c)])\n",
        "\n",
        "\n",
        "class PaddingMask(Layer):\n",
        "    def __call__(self, inputs):\n",
        "        seq = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "        return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "class PaddingAndLookaheadMask(Layer):\n",
        "    def __call__(self, inputs):\n",
        "        size = tf.shape(inputs)[1]\n",
        "        lhm = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "\n",
        "        seq = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "        seq = seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        return tf.maximum(lhm, seq)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}